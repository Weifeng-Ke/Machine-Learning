{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AS6zKmYSwVD"
      },
      "source": [
        "# Programming Assignment 1: Convolution and Back-Propagation\n",
        "\n",
        "**UBC CPEN 455: Deep Learning, 2024 Winter Term 2**\n",
        "\n",
        "**Created By Renjie Liao**\n",
        "\n",
        "**Date: Feb. 19, 2025**\n",
        "\n",
        "**Name: Weifeng Ke**\n",
        "\n",
        "**Student ID: 18879288**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This helps me to mount my google drive onto the colab. I have used chatGPT to help me find the function definitions, like how to create a tensor with zeros,how to do matrix multiplication, and how to do reshape the tensor. Also i have asked chat how to do gradient for the batch normalization, i have ask chat for the theory behind grad batch normalization, how to do back propergation for the CNN."
      ],
      "metadata": {
        "id": "n7krMnhXO9dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# #drive.mount('/content/drive/MyDrive/CPEN_455')"
      ],
      "metadata": {
        "id": "zwZBs3JhONlK"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhakGhuGTBzQ"
      },
      "source": [
        "---\n",
        "# Setup\n",
        "\n",
        "We will use PyTorch to implement this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "PoqjO9gIKMMV"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pdb\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from math import pi\n",
        "from math import floor\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, utils\n",
        "\n",
        "torch.set_default_dtype(torch.float64)\n",
        "\n",
        "## load MNIST images\n",
        "B = 5 # batch size\n",
        "train_set = datasets.MNIST('./',\n",
        "                            train=True,\n",
        "                            download=True,\n",
        "                            transform=transforms.ToTensor())\n",
        "# train_set = datasets.MNIST('/content/drive/MyDrive/CPEN_455/data',\n",
        "#                             train=True,\n",
        "#                             download=False,\n",
        "#                             transform=transforms.ToTensor())\n",
        "loader = torch.utils.data.DataLoader(train_set, batch_size=B)\n",
        "\n",
        "## load a batch of MNIST images as a PyTorch tensor (shape: B x C x H x W)\n",
        "# B: batch size\n",
        "# C: number of channels\n",
        "# H: height of images\n",
        "# W: width of images\n",
        "img, label = next(iter(loader)) # img shape: B x C x H x W, label shape: B X 1\n",
        "\n",
        "## create a random filter (shape: D x C x K x K)\n",
        "K = 3 # kernel size\n",
        "P = 1 # padding size\n",
        "C = 1 # channel size\n",
        "D = 2 # number of filters\n",
        "filter = torch.randn(D, C, K, K) # filter shape: D x C x K x K"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6t6YO5QlYhbX"
      },
      "source": [
        "---\n",
        "# Q1 [60Pts]: 2D convolution and its gradient\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koMGWjFvWWOP"
      },
      "source": [
        "## 1.1 [30Pts]  Implement 2D convolution:\n",
        "\n",
        "Discrete convolution can be implemented in multiple ways, e.g., matrix multiplication in spatial/Fourier domains.\n",
        "\n",
        "First, let us take a look at 1D convolution in spatial domain. Suppose we have a 1D signal with $n$ elements $x_1, x_2, \\dots, x_n$ and a 1D filter with $m$ weights $h_1, h_2, \\dots, h_m$. Note that we typically use filters with odd sizes for the ease of indexing.\n",
        "\n",
        "The (discrete) convolution with zero-padding and stride 1 is defined as:\n",
        "\n",
        "\\begin{align}\n",
        "    y = h \\ast x = \\sum_{i=1}^{n} \\sum_{j=1}^{m} h_j x_{i - \\lfloor m/2 \\rfloor - 1 + j},\n",
        "\\end{align}\n",
        "where padded values $x_{-\\lfloor m/2 \\rfloor + 1}, \\dots, x_{0}, x_{n+1}, \\dots, x_{n - \\lfloor m/2 \\rfloor - 1 + m}$ are all zeros.\n",
        "\n",
        "If you forget about the concepts of padding and stride, take a look at [this guide](https://arxiv.org/pdf/1603.07285.pdf) or [these pictures](https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md).\n",
        "\n",
        "In the 1D case, we can illustrate the two matrix multiplication views of spatial convolution as below.\n",
        "\n",
        "1.   **im2col**:\n",
        "The key idea is to first extract the spatial windows from the signal $x$ for individual convolutions and then perform convolutions (i.e., dot product with the filter).\n",
        "If we put each window as a column in a matrix (the right one in RHS below), then we can perform convolution via the following matrix multiplication (N.B.: the products between the filter and individual columns can be done in parallel).\n",
        "\n",
        "\\begin{align}\n",
        "    y^\\top = (h \\ast x)^{\\top} = \\begin{bmatrix}\n",
        "                h_m & h_{m-1} & \\cdots & h_3 & h_2 & h_1\n",
        "            \\end{bmatrix}\n",
        "            \\begin{bmatrix}\n",
        "                x_{m - \\lfloor m/2 \\rfloor} & x_{m - \\lfloor m/2 \\rfloor + 1} & \\cdots & x_m & x_{m+1} & \\cdots & 0 & 0 \\\\\n",
        "                \\vdots & \\vdots & \\cdots & x_{m-1} & x_m & \\cdots & \\vdots & \\vdots \\\\\n",
        "                x_1 & x_2 & \\cdots & \\vdots & x_{m-1} & \\cdots  & x_n & 0 \\\\\n",
        "                0 & x_1 & \\cdots & \\vdots & \\vdots & \\cdots  & x_{n-1} & x_n \\\\\n",
        "                \\vdots & 0 & \\cdots & \\vdots & \\vdots & \\cdots & \\vdots & \\vdots \\\\                        \n",
        "                0 & 0 & \\cdots & x_1 & x_2 & \\cdots & x_{n - \\lfloor m/2 \\rfloor+1} & x_{n - \\lfloor m/2 \\rfloor}\n",
        "            \\end{bmatrix}.\n",
        "\\end{align}\n",
        "\n",
        "2.   **filter2row**: The key idea is to convert the filter and the signal to a sparse cyclic matrix and a vector respectively.\n",
        "Then the convolution is simply the matrix multiplication between the filter and the signal.\n",
        "\n",
        "\\begin{align}\n",
        "        y = h \\ast x =\n",
        "            \\begin{bmatrix}\n",
        "                h_{\\lfloor m/2 \\rfloor + 1} & h_{\\lfloor m/2 \\rfloor + 2} & \\cdots & h_m & 0 & \\cdots & \\cdots & \\cdots & \\cdots & 0 \\\\\n",
        "                h_{\\lfloor m/2 \\rfloor} & h_{\\lfloor m/2 \\rfloor + 1} & \\cdots & h_{m-1} & h_m & 0 & \\cdots & \\cdots & \\cdots & 0 \\\\\n",
        "                \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
        "                h_1 & h_2 & \\cdots & \\cdots & \\cdots & \\cdots & h_m & 0 & \\cdots & 0 \\\\\n",
        "                0 & h_1 & h_2 & \\cdots & \\cdots & \\cdots & \\cdots & h_m & \\cdots & 0 \\\\\n",
        "                \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
        "                0 & 0 & \\cdots & \\cdots & \\cdots & 0 & h_1 & h_2 & \\cdots & h_{\\lfloor m/2 \\rfloor + 2} \\\\                \n",
        "                0 & 0 & \\cdots & \\cdots & & \\cdots \\cdots & 0 & h_1 & \\cdots & h_{\\lfloor m/2 \\rfloor + 1}\n",
        "            \\end{bmatrix}\n",
        "            \\begin{bmatrix}\n",
        "                x_1 \\\\\n",
        "                x_2 \\\\\n",
        "                x_3 \\\\\n",
        "                \\vdots \\\\\n",
        "                x_n\n",
        "            \\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "**Task**:\n",
        "Implement the 2D convolution in the spatial domain via matrix multiplication following the above two views: **im2col** and **filter2row**.\n",
        "The starter code is provided below.\n",
        "You just need to fill in the missing parts of function ***conv2d_im2col*** and ***conv2d_filter2row***.\n",
        "If your implementation is correct, the ***unit_test*** will output:\n",
        "\n",
        "*Your implementation of xxx is correct!*\n",
        "\n",
        "Otherwise, it will output:\n",
        "\n",
        "*Your implementation of xxx is wrong!*\n",
        "\n",
        "**N.B.**: we assume the strides along height and width are the same and the kernel is square\n",
        "\n",
        "**Hint**: you can reduce the 2D case to 1D and follow the above construction."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# B,C,H,W=img.shape\n",
        "# D,K=filter.shape[0],filter.shape[2]\n",
        "# # img_show=img[0,0,:,:]\n",
        "# # img_see=img_show.squeeze(0)\n",
        "# # plt.imshow(img_see)\n",
        "# # plt.show()\n",
        "# # img[0]\n",
        "# # print(img[0,0,:,:])\n",
        "# img_pad=torch.nn.functional.pad(img, (P, P, P, P), mode='constant', value=0)\n",
        "# #img_pad[0,0,:,:]\n",
        "# print(\"image size:\",img.shape)\n",
        "# print(\"Padded image size :\" ,img_pad.shape)\n",
        "# print(\"filter size:\", filter.shape)\n",
        "# filter_flat=filter.reshape(D,C,K*K)\n",
        "# img_pad_flat=img_pad.reshape(B,C,img_pad.shape[2]*img_pad.shape[3])\n",
        "\n",
        "# print(\"filter_flat size:\", filter_flat.shape)\n",
        "# print(\"img_pad_flat size:\", img_pad_flat.shape)"
      ],
      "metadata": {
        "id": "yxNxvIrBfATZ"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def pre_process(img, filter, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
        "#     B=img.shape[0]\n",
        "#     C=img.shape[1]\n",
        "#     D=filter.shape[0]\n",
        "#     # Pad the image using tensor\n",
        "#     img_padded = torch.nn.functional.pad(img, (padding, padding, padding, padding), mode='constant', value=0) # Use torch.nn.functional.pad for padding\n",
        "#     #img_padded = np.pad(img, pad_width=((0,0), (0,0), (padding, padding), (padding, padding)), mode='constant')\n",
        "#     H_pad, W_pad = img_padded.shape[2], img_padded.shape[3]\n",
        "\n",
        "#     # Calculate the dimensions of the output feature map\n",
        "#     out_h = floor((H_pad - kernel_size) / stride) + 1\n",
        "#     out_w = floor((W_pad - kernel_size) / stride) + 1\n",
        "\n",
        "#     # Initialize an empty array to hold columns (each column is one patch)\n",
        "#     #column size is (B X C X out_h*out_w X C*K*K) ->(B X C X 28*28 X 3*3)\n",
        "#     #cols = np.zeros((B, C, out_h*out_w, kernel_size*kernel_size))\n",
        "\n",
        "#     #Construct the column matrix\n",
        "#     cols= torch.zeros(B, C , out_h*out_w, K*K)\n",
        "#     for b in range(B):\n",
        "#       col_idx=0 # initialize the conlumn index\n",
        "#       #extract the patches\n",
        "#       for i in range(0, H_pad - kernel_size + 1, stride):\n",
        "#           for j in range(0, W_pad - kernel_size + 1, stride):\n",
        "#               # Extract the patch (kernel_size x kernel_size)\n",
        "#               patch = img_padded[b, 0, i:i + kernel_size, j:j + kernel_size]\n",
        "#               # Reshape the patch and assign to the column matrix\n",
        "#               cols[b,0,col_idx] = patch.flatten()\n",
        "#               col_idx += 1  # Increment column index\n",
        "\n",
        "#     cols=cols.reshape(B,C,out_h*out_w,K*K).permute(0,1,3,2)\n",
        "#     print(\"Columns shape is:\",cols.shape)\n",
        "#     cols=cols.reshape(B,C*K*K,out_h*out_w)\n",
        "#     print(\"New Columns shape is:\",cols.shape)\n",
        "\n",
        "#     filter_flat=filter.reshape(D,C*K*K)\n",
        "#     print(\"filter_flat shape is:\",filter_flat.shape)\n",
        "#     #filter dimension (D X C*K*K) times img shape(B X C*K*K X out_h*out_w)\n",
        "#     out=torch.matmul(filter_flat, cols)\n",
        "#     out=out.reshape(B,D,out_h*out_w)\n",
        "#     print(\"out shape is:\",out.shape)\n",
        "\n",
        "#     #print(\"filter flat : \", filter_flat)\n",
        "#     #print(\"filter flat flip : \", filter_flat_flip)\n",
        "\n",
        "# pre_process(img, filter, channel_size=C, num_filters=1, kernel_size=K, stride=1, padding=P)\n",
        "# # display(\"the flat img has size:\",flat_img.shape)\n",
        "# # display(\"The flat filter has size :\",flat_fil.shape)\n",
        "# # flat_img=torch.tensor(flat_img)\n",
        "# # flat_fil=torch.tensor(flat_fil)\n",
        "# # display(\"This is the flatted filter: \",flat_fil)\n",
        "# # display(\"This is the flatted img:\", flat_img)"
      ],
      "metadata": {
        "id": "KXkzmVFP0wiD"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "-IRaDddEdgT0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c6281bc-a216-4041-ac01-fc2825da4fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your implementation of conv2d_im2col is correct!\n",
            "Your implementation of conv2d_filter2row is correct!\n"
          ]
        }
      ],
      "source": [
        "## implement the following two functions\n",
        "def conv2d_im2col(img, filter, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
        "    ### Fill in this function ###\n",
        "    # Args:\n",
        "    #   img: images, shape B x C x H x W\n",
        "    #   filter: filters, shape D x C x K x K\n",
        "    #   channel_size: number of channels, scalar (C)\n",
        "    #   num_filters: number of filters, scalar (D)\n",
        "    #   kernel_size: kernel size, scalar (K)\n",
        "    #   stride: stride size, scalar\n",
        "    #   padding: padding size, scalar\n",
        "    #\n",
        "    # Returns:\n",
        "    #   out: convoluted images, shape B x D x H x W\n",
        "    B,C,H,W=img.shape\n",
        "    D,C_f,K=filter.shape[0],filter.shape[1],filter.shape[2]\n",
        "\n",
        "    # Calculate output spatial dimensions\n",
        "    out_h = floor((H + 2 * padding - kernel_size) / stride) + 1\n",
        "    out_w = floor((W + 2 * padding - kernel_size) / stride) + 1\n",
        "     # Pad the image using a for loop\n",
        "    H_pad, W_pad = H + 2 * padding, W + 2 * padding\n",
        "    img_padded = torch.zeros(B, C, H_pad, W_pad)\n",
        "    for b in range(B):\n",
        "        for c in range(C):\n",
        "            # Copy the original image to the center of the padded image\n",
        "            img_padded[b, c, padding:H_pad - padding, padding:W_pad - padding] = img[b, c]\n",
        "\n",
        "    filter_flat=filter.reshape(D,C*K*K)\n",
        "\n",
        "\n",
        "    #Construct the column matrix\n",
        "    cols= torch.zeros(B, out_h*out_w, C*K*K)\n",
        "    for b in range(B):\n",
        "      col_idx=0 # initialize the conlumn index\n",
        "      #extract the patches\n",
        "      for i in range(0, H_pad - kernel_size + 1, stride):\n",
        "          for j in range(0, W_pad - kernel_size + 1, stride):\n",
        "              # Extract the patch (c,kernel_size x kernel_size)\n",
        "              patch = img_padded[b, :, i:i + kernel_size, j:j + kernel_size]\n",
        "              # Reshape the patch and assign to the column matrix\n",
        "              cols[b,col_idx,:] = patch.reshape(-1)\n",
        "              col_idx += 1  # Increment column index\n",
        "    # Rearrange cols to shape (B, C*K*K, H_out*W_out) by transposing the last two dimensions\n",
        "    cols = cols.permute(0, 2, 1)  # Now shape: (B, C*K*K, H_out*W_out)\n",
        "    # For each image in the batch, perform matrix multiplication:\n",
        "    out = torch.zeros(B, D, out_h * out_w)\n",
        "    for b in range(B):\n",
        "        out[b] = torch.matmul(filter_flat, cols[b])\n",
        "\n",
        "    # Reshape output to (B, D, H_out, W_out)\n",
        "    out = out.reshape(B, D, out_h, out_w)\n",
        "    return out\n",
        "\n",
        "def conv2d_filter2row(img, filter, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
        "    ### Fill in this function ###\n",
        "    # Args:\n",
        "    #   img: images, shape B x C x H x W\n",
        "    #   filter: filters, shape D x C x K x K\n",
        "    #   channel_size: number of channels, scalar (C)\n",
        "    #   num_filters: number of filters, scalar (D)\n",
        "    #   kernel_size: kernel size, scalar (K)\n",
        "    #   stride: stride size, scalar\n",
        "    #   padding: padding size, scalar\n",
        "    #\n",
        "    # Returns:\n",
        "    #   out: convoluted images, shape B x D x H x W\n",
        "\n",
        "    # Calculate output spatial dimensions\n",
        "    B,C,H,W=img.shape\n",
        "    D,C_f,K=filter.shape[0],filter.shape[1],filter.shape[2]\n",
        "\n",
        "    # Calculate output spatial dimensions\n",
        "    out_h = floor((H + 2 * padding - kernel_size) / stride) + 1\n",
        "    out_w = floor((W + 2 * padding - kernel_size) / stride) + 1\n",
        "     # Pad the image using a for loop\n",
        "    H_pad, W_pad = H + 2 * padding, W + 2 * padding\n",
        "    img_padded = torch.zeros(B, C, H_pad, W_pad)\n",
        "    for b in range(B):\n",
        "        for c in range(C):\n",
        "            # Copy the original image to the center of the padded image\n",
        "            img_padded[b, c, padding:H_pad - padding, padding:W_pad - padding] = img[b, c]\n",
        "\n",
        "    # So, we'll initialize cols with shape: (B, H_out*W_out, C*K*K)\n",
        "    cols = torch.zeros(B, out_h * out_w, C * kernel_size * kernel_size, device=img.device, dtype=img.dtype)\n",
        "\n",
        "    #flaten the filter into column matrix shape (C*K*K, D)\n",
        "    filter_row=filter.reshape(D,filter.shape[1]*filter.shape[2]*filter.shape[3]).T\n",
        "    #print(\"filter_row shape is:\",filter_row.shape)\n",
        "\n",
        "    # Pad the image using a for loop\n",
        "    #Construct the row matrix\n",
        "    rows = torch.zeros(B, out_h*out_w, filter.shape[1]*filter.shape[2]*filter.shape[3])\n",
        "    #print(\"Shape of the rows:\", rows.shape)\n",
        "    for b in range(img.shape[0]):\n",
        "      row_idx=0 # initialize the row index\n",
        "      #extract the patches\n",
        "      for i in range(0, H + 2 * padding - kernel_size + 1, stride):\n",
        "          for j in range(0, W + 2 * padding - kernel_size + 1, stride):\n",
        "              # Extract the patch (kernel_size x kernel_size)\n",
        "              patch = img_padded[b, :, i:i + kernel_size, j:j + kernel_size]\n",
        "              # Reshape the patch and assign to the column matrix\n",
        "              rows[b, row_idx,:] = patch.flatten() #reshape to size C*K*K\n",
        "              row_idx += 1  # Increment row index\n",
        "    #print(\"Shape of the filter rows:\", filter_row.shape)\n",
        "    #print(\"Shape of the rows:\", rows.shape)\n",
        "    # For each image in the batch, perform the matrix multiplication:\n",
        "    # (out_h*out_w, C*K*K) dot (C*K*K, D) -> (out_h*out_w, D)\n",
        "    out = torch.matmul(rows, filter_row)\n",
        "\n",
        "    # Reshape to the output (B, out_h, out_w, D)\n",
        "    out = out.reshape(B, out_h, out_w, D).permute(0,3,1,2)\n",
        "    #print(\"size of out\", out.shape)\n",
        "    return out\n",
        "\n",
        "def unit_test_conv2d(img, filter, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
        "    # call your implemented \"im2col\" conv2D\n",
        "    y_im2col = conv2d_im2col(img, filter, channel_size=channel_size, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "    # ground truth conv2D\n",
        "    y_gt = F.conv2d(img, filter, stride=stride, padding=padding)\n",
        "\n",
        "    diff = (y_im2col - y_gt).norm()\n",
        "    if diff < 1.0e-9:\n",
        "        print(\"Your implementation of conv2d_im2col is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of conv2d_im2col is wrong!\")\n",
        "\n",
        "    # call your implemented \"im2col\" conv2D\n",
        "    y_filter2row = conv2d_filter2row(img, filter, channel_size=channel_size, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "    diff = (y_filter2row - y_gt).norm()\n",
        "    if diff < 1.0e-9:\n",
        "        print(\"Your implementation of conv2d_filter2row is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of conv2d_filter2row is wrong!\")\n",
        "\n",
        "\n",
        "unit_test_conv2d(img, filter, channel_size=C, num_filters=D, kernel_size=K, stride=1, padding=P)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l4zm6mEXCxO"
      },
      "source": [
        "## 1.2 [20Pts] Implement the gradient of 2D convolution\n",
        "\n",
        "We now turn to the gradient of 2D convolution.\n",
        "In particular, given a batch of images $x$ with the shape $B \\times C \\times H \\times W$ and filters $h$ with shape $D \\times C \\times K \\times K$, we can view the convolution (zero-padding and stride 1) as a function\n",
        "\\begin{align}\n",
        "    y = f(h, x),\n",
        "\\end{align}\n",
        "that would produce an output tensor $y$ with shape $B \\times D \\times H \\times W$.\n",
        "\n",
        "If we vectorize $x$, $h$, and $y$, then the Jacobian matrix $∇f = [\\frac{\\partial y}{\\partial h}, \\frac{\\partial y}{\\partial x}]$ would be of shape $BDHW \\times (DCKK + BCHW)$.\n",
        "In practice, we almost never need to compute the Jacobian matrix directly as it is unnecessary for back-propagation.\n",
        "Instead, we often need to compute the product between the transposed Jacobian and a vector (a.k.a. vector-Jacobian product), i.e., ${\\frac{\\partial y}{\\partial h}}^{\\top} v$ and ${\\frac{\\partial y}{\\partial x}}^{\\top} v$.\n",
        "For example, the vector $v$ could the gradient of some loss $\\ell$ (scalar) w.r.t. the output above (i.e., $\\frac{\\partial \\ell}{\\partial y}$).\n",
        "\n",
        "**Task**:\n",
        "Given input images $x$, filters $h$, output $y$, and a vector $v$, implement the gradients ${\\frac{\\partial y}{\\partial h}}^{\\top} v$ and ${\\frac{\\partial y}{\\partial x}}^{\\top} v$ in the function below.\n",
        "\n",
        "**N.B.**: The function needs to return the gradients in the original shapes, i.e., ${\\frac{\\partial y}{\\partial h}}^{\\top} v$ should have the same shape as $h$ ($D \\times C \\times K \\times K$) and ${\\frac{\\partial y}{\\partial x}}^{\\top} v$ should have the same shape as $x$ ($B \\times C \\times H \\times W$)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "Vtz-vk95d9WR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dee6c87d-4cbb-4850-fbd5-23fae0b92ae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your implementation of grad_img is correct!\n",
            "Your implementation of grad_filter is correct!\n"
          ]
        }
      ],
      "source": [
        "def im2col(img, kernel_size, stride, padding):\n",
        "    B, C, H, W = img.shape\n",
        "    out_h = (H + 2*padding - kernel_size) // stride + 1\n",
        "    out_w = (W + 2*padding - kernel_size) // stride + 1\n",
        "\n",
        "    # Pad the image using a for loop (as before)\n",
        "    H_pad, W_pad = H + 2 * padding, W + 2 * padding\n",
        "    img_padded = torch.zeros(B, C, H_pad, W_pad, dtype=img.dtype, device=img.device)\n",
        "    for b in range(B):\n",
        "        for c in range(C):\n",
        "            img_padded[b, c, padding:H_pad - padding, padding:W_pad - padding] = img[b, c]\n",
        "\n",
        "    # Construct the column matrix using for loops\n",
        "    cols = torch.zeros(B, C * kernel_size * kernel_size, out_h * out_w, dtype=img.dtype, device=img.device)\n",
        "    for b in range(B):\n",
        "        col_idx = 0\n",
        "        for i in range(0, H_pad - kernel_size + 1, stride):\n",
        "            for j in range(0, W_pad - kernel_size + 1, stride):\n",
        "                # Extract the patch and flatten it\n",
        "                patch = img_padded[b, :, i:i + kernel_size, j:j + kernel_size]  # Extract all channels\n",
        "                cols[b, :, col_idx] = patch.flatten()\n",
        "                col_idx += 1\n",
        "\n",
        "    return cols\n",
        "def col2im(cols, img_shape, kernel_size, stride, padding):\n",
        "    B, C, H, W = img_shape\n",
        "    # Use F.fold to invert the unfolding\n",
        "    H_pad, W_pad = H + 2 * padding, W + 2 * padding\n",
        "    # Initialize the output image with zeros\n",
        "    img_padded = torch.zeros(B, C, H_pad, W_pad, dtype=cols.dtype, device=cols.device)\n",
        "\n",
        "    # Iterate through the columns and place patches back into the image\n",
        "    col_idx = 0\n",
        "    for i in range(0, H_pad - kernel_size + 1, stride):\n",
        "        for j in range(0, W_pad - kernel_size + 1, stride):\n",
        "            # Extract the column and reshape it to a patch\n",
        "            patch = cols[:, :, col_idx].view(B, C, kernel_size, kernel_size)\n",
        "            # Add the patch to the corresponding location in the image\n",
        "            img_padded[:, :, i:i + kernel_size, j:j + kernel_size] += patch\n",
        "            col_idx += 1\n",
        "    # Remove the extra padding\n",
        "    if padding > 0:\n",
        "        img = img_padded[:, :, padding:-padding, padding:-padding]\n",
        "    else:\n",
        "        img = img_padded\n",
        "    return img\n",
        "\n",
        "# implement the following functions\n",
        "def grad_conv2d(img, filter, out, grad_out, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
        "    ### Fill in this function ###\n",
        "    # Args:\n",
        "    #   img: images, shape B x C x H x W\n",
        "    #   filter: filters, shape D x C x K x K\n",
        "    #   out: convoluted images, shape B x D x H x W\n",
        "    #   grad_out: gradient w.r.t. output, shape B x D x H x W\n",
        "    #   channel_size: number of channels, scalar (C)\n",
        "    #   num_filters: number of filters, scalar (D)\n",
        "    #   kernel_size: kernel size, scalar (K)\n",
        "    #   stride: stride size, scalar\n",
        "    #   padding: padding size, scalar\n",
        "    #\n",
        "    # Returns:\n",
        "    #   grad_img: gradient w.r.t. img, shape B x C x H x W\n",
        "    #   grad_filer: gradient w.r.t. filter, shape D x C x K x K\n",
        "\n",
        "    #getting the right shape\n",
        "    B, C, H, W = img.shape\n",
        "    D, C_f, K, K2 = filter.shape\n",
        "    # Compute output spatial dimensions.\n",
        "    out_h = (H + 2*padding - kernel_size) // stride + 1\n",
        "    out_w = (W + 2*padding - kernel_size) // stride + 1\n",
        "\n",
        "\n",
        "    # Compute gradient w.r.t. filters\n",
        "    # Unroll input using im2col: shape (B, C*K*K, out_h*out_w)\n",
        "    cols = im2col(img, kernel_size, stride, padding)\n",
        "    #print(\"im2col output shape: \",cols.shape)\n",
        "    # Reshape grad_out: (B, D, out_h*out_w)\n",
        "    grad_out_reshaped = grad_out.view(B, D, -1)\n",
        "\n",
        "    # We want grad_filter of shape (D, C*K*K). For each image:\n",
        "    # grad_filter += grad_out[b] (D x (out_h*out_w)) @ cols[b].T ((out_h*out_w) x (C*K*K))\n",
        "    grad_filter = torch.zeros(D, C * kernel_size * kernel_size, device=img.device, dtype=img.dtype)\n",
        "    for b in range(B):\n",
        "        grad_filter += torch.matmul(grad_out_reshaped[b], cols[b].transpose(0,1))\n",
        "    # Reshape to (D, C, K, K)\n",
        "    grad_filter = grad_filter.view(D, C, kernel_size, kernel_size)\n",
        "\n",
        "    # --- Compute gradient w.r.t. input images ---\n",
        "    # Reshape filters to (D, C*K*K)\n",
        "    filt_reshaped = filter.view(D, -1)\n",
        "    # For each image, compute the column form gradient:\n",
        "    # grad_img_cols = (filt_reshapedᵀ) (shape: (C*K*K, D)) @ grad_out[b] (D x (out_h*out_w))\n",
        "    grad_img_cols = torch.matmul(filt_reshaped.transpose(0,1), grad_out_reshaped)\n",
        "    # grad_img_cols has shape (B, C*K*K, out_h*out_w)\n",
        "    # Fold the columns back into the image using col2im\n",
        "    grad_img = col2im(grad_img_cols, img.shape, kernel_size, stride, padding)\n",
        "\n",
        "    return grad_img, grad_filter\n",
        "\n",
        "def unit_test_grad_conv2d(img, filter, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
        "    filter.requires_grad = True\n",
        "    img.requires_grad = True\n",
        "\n",
        "    ### ground truth conv2D\n",
        "    img_out = F.conv2d(img, filter, stride=stride, padding=padding)\n",
        "\n",
        "    # create a random vector v\n",
        "    v = torch.randn_like(img_out)\n",
        "\n",
        "    # call your implemented \"grad_conv2d\" function\n",
        "    grad_img, grad_filter = grad_conv2d(img, filter, img_out, v, channel_size=channel_size, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "    # compute ground-truth gradients\n",
        "    grad_img_gt = torch.autograd.grad(img_out, img, grad_outputs=v, retain_graph=True)[0]\n",
        "    grad_filter_gt = torch.autograd.grad(img_out, filter, grad_outputs=v, retain_graph=True)[0]\n",
        "    #pdb.set_trace()\n",
        "\n",
        "    diff = (grad_img - grad_img_gt).norm()\n",
        "    if diff < 1.0e-5:\n",
        "        print(\"Your implementation of grad_img is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of grad_img is wrong!\")\n",
        "\n",
        "    diff = (grad_filter - grad_filter_gt).norm()\n",
        "    if diff < 1.0e-5:\n",
        "        print(\"Your implementation of grad_filter is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of grad_filter is wrong!\")\n",
        "\n",
        "unit_test_grad_conv2d(img, filter, channel_size=C, num_filters=D, kernel_size=K, stride=1, padding=P)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QI9J5CDCUSCc"
      },
      "source": [
        "---\n",
        "## 1.3 [10Pts]: Implement gradient checking via the finite difference approximation\n",
        "\n",
        "We verify the correctness of the implementation of gradient operators by calling PyTorch's autograd function.\n",
        "However, PyTorch's autograd function just calls the gradient operators implemented by the PyTorch team.\n",
        "How do they verify the correctness of their implementation?\n",
        "\n",
        "The answer is **finite difference approximation**.\n",
        "Following the setup in 1.2, given a batch of images $x$ with the shape $B \\times C \\times H \\times W$ and filters $h$ with shape $D \\times C \\times K \\times K$, we have the convolution (zero-padding and stride 1)\n",
        "\\begin{align}\n",
        "    y = f(h, x).\n",
        "\\end{align}\n",
        "\n",
        "Again, mentally vectorize $x$ and $y$ would help us understand the math.\n",
        "Given any vector $v$ with the same shape as $y$, we are interested in computing ${\\frac{\\partial y}{\\partial h}}^{\\top} v$ and ${\\frac{\\partial y}{\\partial x}}^{\\top} v$.\n",
        "These two gradients are equivalent to ${\\frac{\\partial \\ell}{\\partial h}}$ and ${\\frac{\\partial \\ell}{\\partial x}}$ where\n",
        "\\begin{align}\n",
        "    \\ell(h, x) = y^{\\top}v = f(h, x)^{\\top} v.\n",
        "\\end{align}\n",
        "Note that here $\\ell$ becomes a scalar.\n",
        "Based on Talyor's theorem, we have\n",
        "\\begin{align}\n",
        "    d^{\\top} \\frac{\\partial \\ell}{\\partial h} = \\lim_{\\epsilon → 0} \\frac{\\ell(h + \\epsilon \\cdot d, x) - \\ell(h - \\epsilon \\cdot d, x)}{2 ϵ},\n",
        "\\end{align}\n",
        "where $d$ could be any direction vector and $ϵ$ is a scalar.\n",
        "For our purpose, we just need to set $d$ to be the unit vector to compute the per-dimension value of $\\frac{\\partial \\ell}{\\partial h}$.\n",
        "Specifically, if we set $d$ as the $i$-th unit vector $e_i$, i.e., $d[i] = 1$ and $d[j] = 0, \\forall j \\neq i$, we can then compute\n",
        "\\begin{align}\n",
        "    \\frac{\\partial \\ell}{\\partial h}[i] &= \\lim_{\\epsilon → 0} \\frac{\\ell(h + \\epsilon \\cdot e_i, x) - \\ell(h - \\epsilon \\cdot e_i, x)}{2 ϵ} \\\\\n",
        "    & ≈ \\frac{\\ell(h + \\epsilon \\cdot e_i, x) - \\ell(h - \\epsilon \\cdot e_i, x)}{2 ϵ}.\n",
        "\\end{align}\n",
        "\n",
        "**Task**: Implement the finite-difference based gradient checker for ${\\frac{\\partial \\ell}{\\partial h}}$ and ${\\frac{\\partial \\ell}{\\partial x}}$.\n",
        "\n",
        "\n",
        "**N.B.**: For efficiency consideration in the unit test, you can use F.conv2d to compute the convolution in your implementation of *grad_checker*. This assignment is to let you understand how to implement finte-difference. But in pratice, if we want to verify our implementation of conv2d, then we should use our conv2d instead of F.conv2d from PyTorch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "qgeHgGn1JbBn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e657d66-d17a-41f9-8b6f-3c6be7bf0712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your implementation of grad_img is correct!\n",
            "Your implementation of grad_filter is correct!\n"
          ]
        }
      ],
      "source": [
        "## implement the following functions\n",
        "def loss_fn(img, filter, conv, grad_out):\n",
        "    y = conv(img, filter,padding=1)\n",
        "    loss = torch.sum(y * grad_out)\n",
        "    return loss\n",
        "def grad_checker(img, filter, conv, grad_out, epsilon=1.0e-5, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
        "    ### Fill in this function ###\n",
        "    # Args:\n",
        "    #   img: images, shape B x C x H x W\n",
        "    #   filter: filters, shape D x C x K x K\n",
        "    #   conv: convolution function\n",
        "    #   out: convoluted images, shape B x D x H x W\n",
        "    #   grad_out: gradient w.r.t. output, shape B x D x H x W\n",
        "    #   channel_size: number of channels, scalar (C)\n",
        "    #   num_filters: number of filters, scalar (D)\n",
        "    #   kernel_size: kernel size, scalar (K)\n",
        "    #   stride: stride size, scalar\n",
        "    #   padding: padding size, scalar\n",
        "    #\n",
        "    # Returns:\n",
        "    #   grad_img: gradient w.r.t. img, shape B x C x H x W\n",
        "    #   grad_filer: gradient w.r.t. filter, shape D x C x K x K\n",
        "\n",
        "    img_og=img.clone()\n",
        "    filter_og=filter.clone()\n",
        "\n",
        "    # Initialize numerical gradients with zeros.\n",
        "    grad_filter_fd = torch.zeros_like(filter)\n",
        "    grad_img_fd    = torch.zeros_like(img)\n",
        "  \t# Gradient with respect to filter, Flatten filter for easier indexing.\n",
        "    filt_flat = filter.view(-1)\n",
        "    for i in range(filt_flat.numel()):\n",
        "        # Create a perturbation vector for filt.\n",
        "        perturb = torch.zeros_like(filt_flat)\n",
        "        perturb[i] = epsilon\n",
        "\n",
        "        # Evaluate loss for filt + epsilon*e_i\n",
        "        l_plus = loss_fn(img_og, (filt_flat + perturb).view_as(filter), conv, grad_out)\n",
        "        # Evaluate loss for filt - epsilon*e_i\n",
        "        l_minus = loss_fn(img_og, (filt_flat - perturb).view_as(filter), conv, grad_out)\n",
        "\n",
        "        # Finite difference approximation.\n",
        "        grad_filter_fd.view(-1)[i] = (l_plus - l_minus) / (2 * epsilon)\n",
        "    # --- Gradient with respect to input image ---\n",
        "    # Flatten img for easier indexing.\n",
        "    img_flat = img.view(-1)\n",
        "    for i in range(img_flat.numel()):\n",
        "        # Create a perturbation vector for img.\n",
        "        perturb = torch.zeros_like(img_flat)\n",
        "        perturb[i] = epsilon\n",
        "\n",
        "        # Evaluate loss for img + epsilon*e_i\n",
        "        l_plus = loss_fn((img_flat + perturb).view_as(img), filter_og, conv, grad_out)\n",
        "        # Evaluate loss for img - epsilon*e_i\n",
        "        l_minus = loss_fn((img_flat - perturb).view_as(img), filter_og, conv, grad_out)\n",
        "\n",
        "        # Finite difference approximation.\n",
        "        grad_img_fd.view(-1)[i] = (l_plus - l_minus) / (2 * epsilon)\n",
        "\n",
        "    # Return the numerical gradients (convert back to float if desired)\n",
        "    return grad_img_fd.float(), grad_filter_fd.float()\n",
        "\n",
        "def unit_test_grad_checker(img, filter, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
        "    epsilon = 1.0e-5\n",
        "    filter.requires_grad = True\n",
        "    img.requires_grad = True\n",
        "\n",
        "    ### ground truth conv2D\n",
        "    img_out = F.conv2d(img, filter, stride=stride, padding=padding)\n",
        "\n",
        "    # create a random vector v\n",
        "    v = torch.randn_like(img_out)\n",
        "\n",
        "    # call your implemented \"grad_checker\" function\n",
        "    grad_img, grad_filter = grad_checker(img, filter, F.conv2d, v, epsilon=epsilon, channel_size=channel_size, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "    # compute ground-truth gradients\n",
        "    grad_img_gt = torch.autograd.grad(img_out, img, grad_outputs=v, retain_graph=True)[0]\n",
        "    grad_filter_gt = torch.autograd.grad(img_out, filter, grad_outputs=v, retain_graph=True)[0]\n",
        "    #pdb.set_trace()\n",
        "\n",
        "    diff = (grad_img - grad_img_gt).norm()\n",
        "    if diff < 1.0e-5:\n",
        "        print(\"Your implementation of grad_img is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of grad_img is wrong!\")\n",
        "\n",
        "    diff = (grad_filter - grad_filter_gt).norm()\n",
        "    if diff < 1.0e-5:\n",
        "        print(\"Your implementation of grad_filter is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of grad_filter is wrong!\")\n",
        "\n",
        "unit_test_grad_checker(img, filter, channel_size=C, num_filters=D, kernel_size=K, stride=1, padding=P)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ3DtRlwUlzY"
      },
      "source": [
        "---\n",
        "#Q2 [5Pts]: Implement ReLU and its gradient\n",
        "\n",
        "**Task**: Implement ReLU operator, i.e., $f(x) = max(x, 0)$, and its gradient operator ${\\frac{\\partial f}{\\partial x}}^{\\top} v$ for any given tensor $v$ that is of the same shape as $x$.\n",
        "\n",
        "**N.B.**: For simplicity, we can assume the input $x$ is of shape $B \\times C \\times H \\times W$ as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "_ruiFoTsJb9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50159c98-6796-4733-d9c3-932d91f2d040"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your implementation of func_relu is correct!\n",
            "Your implementation of grad_relu is correct!\n"
          ]
        }
      ],
      "source": [
        "## implement the following functions\n",
        "def func_relu(x):\n",
        "    ### Fill in this function ###\n",
        "    # Args:\n",
        "    #   x: input, shape B x C x H x W\n",
        "    #\n",
        "    # Returns:\n",
        "    #   y: output, shape B x C x H x W\n",
        "    B,C,H,W=x.shape\n",
        "    y=torch.zeros(B,C,H,W)\n",
        "    for i in range(B):\n",
        "      for j in range(C):\n",
        "        for k in range(H):\n",
        "          for l in range(W):\n",
        "            if x[i,j,k,l]>0:\n",
        "              y[i,j,k,l]=x[i,j,k,l]\n",
        "            else:\n",
        "              y[i,j,k,l]=0\n",
        "    return y\n",
        "\n",
        "def grad_relu(x, y, grad_out):\n",
        "    ### Fill in this function ###\n",
        "    # Args:\n",
        "    #   x: input, shape B x C x H x W\n",
        "    #   y: output, shape B x C x H x W\n",
        "    #   grad_out: gradient w.r.t. output y, shape B x D x H x W\n",
        "    #\n",
        "    # Returns:\n",
        "    #   grad_x: gradient w.r.t. x, shape B x C x H x W\n",
        "\n",
        "    #compute the gradient for y=relu(X) if x> 0 gradient of x = 1* grad_out\n",
        "    #del loss/del y * del y/del x* del Relu(x)/x\n",
        "    B,C,H,W=x.shape\n",
        "    grad_x=torch.zeros(B,C,H,W)\n",
        "    for i in range(B):\n",
        "      for j in range(C):\n",
        "        for k in range(H):\n",
        "          for l in range(W):\n",
        "            if x[i,j,k,l]>0.0:\n",
        "              grad_x[i,j,k,l]=1*grad_out[i,j,k,l]\n",
        "            else:\n",
        "              grad_x[i,j,k,l]=0.0\n",
        "    return grad_x\n",
        "# def grad_relu(x, y, grad_out):\n",
        "#     return grad_out * (x > 0).float()\n",
        "def unit_test_relu(x):\n",
        "    x.requires_grad = True\n",
        "\n",
        "    # call your implemented \"func_relu\" function\n",
        "    y = func_relu(x)\n",
        "\n",
        "    # ground truth ReLU\n",
        "    y_gt = F.relu(x)\n",
        "\n",
        "    diff = (y - y_gt).norm()\n",
        "    if diff < 1.0e-5:\n",
        "        print(\"Your implementation of func_relu is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of func_relu is wrong!\")\n",
        "\n",
        "    # create a random vector v\n",
        "    v = torch.randn_like(y)\n",
        "\n",
        "    # call your implemented \"grad_relu\" function\n",
        "    grad_x = grad_relu(x, y, v)\n",
        "\n",
        "    # compute ground-truth gradients\n",
        "    grad_x_gt = torch.autograd.grad(y_gt, x, grad_outputs=v, retain_graph=True)[0]\n",
        "\n",
        "    diff = (grad_x - grad_x_gt).norm()\n",
        "    if diff < 1.0e-5:\n",
        "        print(\"Your implementation of grad_relu is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of grad_relu is wrong!\")\n",
        "\n",
        "unit_test_relu(torch.randn_like(img))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiVr7-xJU066"
      },
      "source": [
        "---\n",
        "#Q3 [20Pts]: Implement Batch-normalization (BN) for convolution and its gradient\n",
        "\n",
        "Given a batch of input images $x$ with shape $B \\times C \\times H \\times W$, we compute a single mean and a single standard deviation per channel as below,\n",
        "\\begin{align}\n",
        "    \\mu[c] &= \\frac{1}{BHW} \\sum_{i=1}^{B} \\sum_{m=1}^{H} \\sum_{n=1}^{W} x[i, c, m, n] \\\\\n",
        "    \\sigma^2[c] &= \\frac{1}{BHW} \\sum_{i=1}^{B} \\sum_{m=1}^{H} \\sum_{n=1}^{W} (x[i, c, m, n] - \\mu[c])^2.\n",
        "\\end{align}\n",
        "Then we perform BN, $y = f(x, \\beta, \\gamma)$, as,\n",
        "\\begin{align}\n",
        "    y[i,c,m,n] &= \\gamma[c] \\frac{x[i,c,m,n] - \\mu[c]}{\\sqrt{\\sigma^2[c] + \\epsilon}} + \\beta[c],\n",
        "\\end{align}\n",
        "where $\\gamma$ and $\\beta$ are learnable parameters are of shape $C$.\n",
        "$ϵ$ is a constant.\n",
        "\n",
        "**Task**: For simplicity, we fix the learnable parameters as $\\gamma = 1$ and $\\beta = 0$.\n",
        "Implement BN for convolution and its gradient operators ${\\frac{\\partial f}{\\partial x}}^{\\top} v$ for any $v$ that is compatible with the matrix multiplication.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "3Sou_6LzJcxS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0ac94d7-4d82-4180-8299-79cf8671d267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your implementation of func_batch_norm is correct!\n",
            "Your implementation of grad_batch_norm is correct!\n"
          ]
        }
      ],
      "source": [
        "## implement the following functions\n",
        "def func_batch_norm(x, epsilon=1.0e-5):\n",
        "    ### Fill in this function ###\n",
        "    # Args:\n",
        "    #   x: input, shape B x C x H x W\n",
        "    #   epsilon: constant, scalar\n",
        "    #\n",
        "    # Returns:\n",
        "    #   y: output, shape B x C x H x W\n",
        "    B,C,H,W=x.shape\n",
        "    u=torch.zeros(C)\n",
        "    sigma2=torch.zeros(C)\n",
        "    y=torch.zeros(B,C,H,W)\n",
        "    gamma=1\n",
        "    beta=0\n",
        "    avg=1/(B*H*W)\n",
        "    #compute the means\n",
        "    for c in range(C):\n",
        "      for i in range(B):\n",
        "        for m in range(H):\n",
        "          for n in range(W):\n",
        "            u[c]+=x[i,c,m,n]\n",
        "      u[c]=avg*u[c]\n",
        "\n",
        "      # Calculate sigma2[c]\n",
        "      for i in range(B):\n",
        "          for m in range(H):\n",
        "              for n in range(W):\n",
        "                  sigma2[c] += (x[i, c, m, n] - u[c])**2\n",
        "      sigma2[c] = avg * sigma2[c]\n",
        "\n",
        "      # Perform Batch Normalization\n",
        "      for i in range(B):\n",
        "          for m in range(H):\n",
        "              for n in range(W):\n",
        "                  y[i, c, m, n] = (gamma * (x[i, c, m, n] - u[c]) / torch.sqrt(sigma2[c] + epsilon))+beta\n",
        "    return y\n",
        "\n",
        "def grad_batch_norm(x, y, grad_out, epsilon=1.0e-5):\n",
        "    ### Fill in this function ###\n",
        "    # Args:\n",
        "    #   x: input, shape B x C x H x W\n",
        "    #   y: output, shape B x C x H x W\n",
        "    #   grad_out: gradient w.r.t. output y, shape B x D x H x W\n",
        "    #   epsilon: constant, scalar\n",
        "    #\n",
        "    # Returns:\n",
        "    #   grad_x: gradient w.r.t. x, shape B x C x H x W\n",
        "    #   grad_x=del loss / del x\n",
        "    # Step 1: Gather the dimensions of the input\n",
        "\n",
        "    # 1. Get dimensions and compute N = number of elements per channel.\n",
        "    B, C, H, W = x.shape\n",
        "    N = B * H * W  # Number of elements per channel\n",
        "    mean=torch.zeros(C)\n",
        "    var=torch.zeros(C)\n",
        "    grad_x=torch.zeros_like(x)\n",
        "    # 2. Compute the mean μ and variance σ² for each channel (across batch, height, width).\n",
        "    for c in range(C):\n",
        "        s = 0.0\n",
        "        for b in range(B):\n",
        "            for i in range(H):\n",
        "                for j in range(W):\n",
        "                    s += x[b, c, i, j]\n",
        "        mean[c] = s / N\n",
        "\n",
        "        # Now compute variance for channel c.\n",
        "        s_sq = 0.0\n",
        "        for b in range(B):\n",
        "            for i in range(H):\n",
        "                for j in range(W):\n",
        "                    diff = x[b, c, i, j] - mean[c]\n",
        "                    s_sq += diff * diff\n",
        "        var[c] = s_sq / N\n",
        "\n",
        "    # 3. Compute the standard deviation with epsilon\n",
        "    SD = torch.sqrt(var + epsilon)\n",
        "    S1 = torch.zeros(C)\n",
        "    S2 = torch.zeros(C)\n",
        "\n",
        "    for c in range(C):\n",
        "        for b in range(B):\n",
        "            for i in range(H):\n",
        "                for j in range(W):\n",
        "                    # delta is the lost wrt y gradient at this element.\n",
        "                    delta = grad_out[b, c, i, j]\n",
        "                    S1[c] += delta\n",
        "                    # x_hat = (x - mean) / std for channel c. formalize imput x_hat\n",
        "                    x_hat = (x[b, c, i, j] - mean[c]) / SD[c]\n",
        "\n",
        "                    S2[c] += delta * x_hat\n",
        "\n",
        "\n",
        "    for c in range(C):\n",
        "        for b in range(B):\n",
        "            for i in range(H):\n",
        "                for j in range(W):\n",
        "                    delta = grad_out[b, c, i, j]\n",
        "                    x_hat = (x[b, c, i, j] - mean[c]) / SD[c]\n",
        "                    grad_val = (delta - S1[c]/N - x_hat * (S2[c]/N)) / SD[c]\n",
        "                    grad_x[b, c, i, j] = grad_val\n",
        "\n",
        "    return grad_x\n",
        "\n",
        "def unit_test_batch_norm(x):\n",
        "    x.requires_grad = True\n",
        "    epsilon = 1e-5\n",
        "\n",
        "    # call your implemented \"func_batch_norm\" function\n",
        "    y = func_batch_norm(x, epsilon=epsilon)\n",
        "\n",
        "    # ground truth ReLU\n",
        "    BN_gt = nn.BatchNorm2d(x.shape[1], eps=epsilon, momentum=1.0, affine=False, track_running_stats=False)\n",
        "    y_gt = BN_gt(x)\n",
        "\n",
        "    diff = (y - y_gt).norm()\n",
        "    if diff < 1.0e-5:\n",
        "        print(\"Your implementation of func_batch_norm is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of func_batch_norm is wrong!\")\n",
        "\n",
        "    # create a random vector v\n",
        "    v = torch.randn_like(y)\n",
        "\n",
        "    # call your implemented \"grad_batch_norm\" function\n",
        "    grad_x = grad_batch_norm(x, y, v, epsilon=epsilon)\n",
        "\n",
        "    # compute ground-truth gradients\n",
        "    grad_x_gt = torch.autograd.grad(y_gt, x, grad_outputs=v, retain_graph=True)[0]\n",
        "\n",
        "    diff = (grad_x - grad_x_gt).norm()\n",
        "    if diff < 1.0e-5:\n",
        "        print(\"Your implementation of grad_batch_norm is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of grad_batch_norm is wrong!\")\n",
        "\n",
        "unit_test_batch_norm(torch.randn_like(img))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2m-KuDI7U2zb"
      },
      "source": [
        "---\n",
        "#Q4 [15Pts]: Implement a simple CNN and back-propagation (BP)\n",
        "\n",
        "Now we are ready to build a deep CNN and learn it with back-propagation.\n",
        "In particular, let us build a simple CNN with following architecture:\n",
        "\n",
        "Conv $→$ BN $→$ ReLU $→$ Conv $→$ BN $→$ ReLU $→$ Linear.\n",
        "\n",
        "Here, for all layers, the convolutions are the same as before (i.e., kernel size $3 \\times 3$, zero-padding, number of filters $D = 2$, and stride 1), the BNs are without learnable $\\gamma$ and $\\beta$, and the last linear layer would map whatever input dimension to $10$ classes in MNIST.\n",
        "\n",
        "**Task**: Implement the above CNN, compute the cross-entropy loss, and compute gradient of the loss w.r.t. filter weights.\n",
        "\n",
        "**N.B.**: You can use F.cross_entropy provided by PyTorch. But for other operators like Conv, BN, and ReLU and their gradietns, you should use your previous implementations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "OnpM4Yj_JdVO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbda2b65-57b7-4c9f-9c2f-f67ef69f6872"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your implementation of grad_filter_1 is correct!\n",
            "Your implementation of grad_filter_2 is correct!\n",
            "Your implementation of grad_weight is correct!\n"
          ]
        }
      ],
      "source": [
        "## implement the following two functions\n",
        "def CNN(img, filter_1, filter_2, weight, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
        "    ### Fill in this function ###\n",
        "    # Args:\n",
        "    #   img: images, shape B x C x H x W\n",
        "    #   filter_1: filters at 1st layer, shape D x C x K x K\n",
        "    #   filter_2: filters at 2nd layer, shape D x C x K x K\n",
        "    #   weight: weights of linear readout layer, shape ? x 10\n",
        "    #   channel_size: number of channels, scalar (C)\n",
        "    #   num_filters: number of filters, scalar (D)\n",
        "    #   kernel_size: kernel size, scalar (K)\n",
        "    #   stride: stride size, scalar\n",
        "    #   padding: padding size, scalar\n",
        "    #\n",
        "    # Returns:\n",
        "    #   out: logits, shape B x 10\n",
        "\n",
        "    B,C,H,W=img.shape\n",
        "    D_1,C_1,K,K=filter_1.shape\n",
        "    D_2,C_2,_,_=filter_2.shape\n",
        "    classes=weight.shape[1]\n",
        "    out=torch.zeros(B,classes)\n",
        "\n",
        "    ### 1st layer\n",
        "    layer_1=conv2d_im2col(img,filter_1,channel_size=C_1,num_filters=D_1,kernel_size=3,stride=1,padding=1)\n",
        "    layer_1=func_batch_norm(layer_1)\n",
        "    layer_1=func_relu(layer_1)\n",
        "    # The output of the previous layer is layer_1 which after flattening has shape (B, D_1 * out_h * out_w).\n",
        "    # Given B=5, D=2, K=3, P=1, S=1, H=28, W=28,\n",
        "    # out_h = floor((H + 2*P - K) / S) + 1 = floor((28 + 2*1 - 3) / 1) + 1 = 28\n",
        "    # out_w = floor((W + 2*P - K) / S) + 1 = floor((28 + 2*1 - 3) / 1) + 1 = 28\n",
        "    #The layer 1 output shape is (B,D_1,28*28)=(5,2,28,28)\n",
        "    #print(\"layer 1 shape\",layer_1.shape)\n",
        "\n",
        "    ### 2nd layer\n",
        "    layer_2=conv2d_im2col(layer_1,filter_2,channel_size=C_2,num_filters=D_2,kernel_size=3,stride=1,padding=1)\n",
        "    # The output of the previous layer is layer_2 which after flattening has shape (B, D * out_h * out_w).\n",
        "    # Given B=5, D=2, K=3, P=1, S=1, H=28, W=28,\n",
        "    # out_h = floor((H + 2*P - K) / S) + 1 = floor((28 + 2*1 - 3) / 1) + 1 = 28\n",
        "    # out_w = floor((W + 2*P - K) / S) + 1 = floor((28 + 2*1 - 3) / 1) + 1 = 28\n",
        "    #the output of layer_2 has shape of (5,2,28,28)\n",
        "    layer_2=func_batch_norm(layer_2)\n",
        "    layer_2=func_relu(layer_2)\n",
        "    #print(\"layer 2 shape\",layer_2.shape)\n",
        "\n",
        "    ### linear readout\n",
        "    #flatten the input\n",
        "    #after flattening the input shape of (5,2,28,28)= (5,2*28*28)=(5,1568)\n",
        "    linear_layer=layer_2.reshape(B,D_2*H*W)\n",
        "    #print(\"linear layer shape\",linear_layer.shape)\n",
        "    #so to make the multiplication work (5 x 1568) @ (1568 , 10)\n",
        "    out=torch.matmul(linear_layer,weight)\n",
        "    #print(\"out shape\",out.shape)\n",
        "\n",
        "\n",
        "    return out\n",
        "\n",
        "def grad_CNN(img, filter_1, filter_2, weight, grad_loss, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
        "    B, C, H, W = img.shape\n",
        "\n",
        "    ### 1st layer forward\n",
        "    layer_1 = conv2d_im2col(img, filter_1, channel_size=filter_1.shape[1], num_filters=filter_1.shape[0],\n",
        "                            kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "    #print(\"layer 1 shape\",layer_1.shape)\n",
        "    bn_layer1 = func_batch_norm(layer_1)\n",
        "    #print(\"bn layer 1 shape\",bn_layer1.shape)\n",
        "    relu_layer1 = func_relu(bn_layer1)\n",
        "    #print(\"relu layer 1 shape\",relu_layer1.shape)\n",
        "\n",
        "    ### 2nd layer forward\n",
        "    layer_2 = conv2d_im2col(relu_layer1, filter_2, channel_size=filter_2.shape[1], num_filters=filter_2.shape[0],\n",
        "                            kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "    #print(\"layer 2 shape\",layer_2.shape)\n",
        "    bn_layer2 = func_batch_norm(layer_2)\n",
        "    #print(\"bn layer 2 shape\",bn_layer2.shape)\n",
        "    relu_layer2 = func_relu(bn_layer2)\n",
        "    #print(\"relu layer 2 shape\",relu_layer2.shape)\n",
        "\n",
        "    ### linear readout forward\n",
        "    linear_layer = relu_layer2.reshape(B, filter_2.shape[0]*img.shape[2]*img.shape[3])\n",
        "    #print(\"linear layer shape\",linear_layer.shape)\n",
        "    out = torch.matmul(linear_layer, weight)\n",
        "    #print(\"out shape\",out.shape)\n",
        "\n",
        "    ### Backward pass----------------------------------------\n",
        "    # Gradient w.r.t. weight\n",
        "    grad_weight = torch.matmul(linear_layer.T, grad_loss)\n",
        "    #print(\"grad weight shape\",grad_weight.shape)\n",
        "\n",
        "    # Gradient w.r.t. linear layer input\n",
        "    grad_linear = grad_loss.matmul(weight.T)\n",
        "    #print(\"grad linear shape\",grad_linear.shape)\n",
        "\n",
        "    # Backprop through linear layer\n",
        "    D2 = filter_2.shape[0]\n",
        "    grad_relu_layer2 = grad_linear.reshape(B, D2, H, W)\n",
        "    #print(\"grad relu layer 2 shape\",grad_relu_layer2.shape)\n",
        "\n",
        "    # Backprop through second ReLU\n",
        "    grad_bn_layer2 = grad_relu(bn_layer2, relu_layer2, grad_relu_layer2)\n",
        "    #print(\"grad bn layer 2 shape\",grad_bn_layer2.shape)\n",
        "\n",
        "    # Backprop through second BN (corrected argument order)\n",
        "    grad_layer2 = grad_batch_norm(layer_2, bn_layer2, grad_bn_layer2)\n",
        "    #print(\"grad layer 2 shape\",grad_layer2.shape)\n",
        "\n",
        "    # Backprop through second conv\n",
        "    grad_relu_layer1, grad_filter_2 = grad_conv2d(relu_layer1, filter_2, layer_2, grad_layer2,\n",
        "                                                  channel_size=filter_2.shape[1], num_filters=filter_2.shape[0],\n",
        "                                                  kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "    #print(\"grad relu layer 1 shape\",grad_relu_layer1.shape)\n",
        "    #print(\"grad filter 2 shape\",grad_filter_2.shape)\n",
        "\n",
        "    # Backprop through first ReLU\n",
        "    grad_bn_layer1 = grad_relu(bn_layer1, relu_layer1, grad_relu_layer1)\n",
        "    #print(\"grad bn layer 1 shape\",grad_bn_layer1.shape)\n",
        "\n",
        "    # Backprop through first BN (corrected argument order)\n",
        "    grad_layer1 = grad_batch_norm(layer_1, bn_layer1, grad_bn_layer1)\n",
        "    #print(\"grad layer 1 shape\",grad_layer1.shape)\n",
        "\n",
        "    # Backprop through first conv\n",
        "    grad_img, grad_filter_1 = grad_conv2d(img, filter_1, layer_1, grad_layer1,\n",
        "                                          channel_size=filter_1.shape[1], num_filters=filter_1.shape[0],\n",
        "                                          kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "    #print(\"grad img shape\",grad_img.shape)\n",
        "    #print(\"grad filter 1 shape\",grad_filter_1.shape)\n",
        "\n",
        "    # Return gradients\n",
        "    return grad_filter_1, grad_filter_2, grad_weight\n",
        "\n",
        "def unit_test_CNN(img, label, filter_1, filter_2, weight, channel_size=1, num_filters=1, kernel_size=3, stride=1, padding=1):\n",
        "    # call your implemented \"CNN\"\n",
        "    img.requires_grad_()\n",
        "    filter_1.requires_grad_()\n",
        "    filter_2.requires_grad_()\n",
        "    weight.requires_grad_()\n",
        "    y = CNN(img, filter_1, filter_2, weight, channel_size=channel_size, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "    y.requires_grad_()\n",
        "\n",
        "    # compute loss function\n",
        "    loss = F.cross_entropy(y, label).mean()\n",
        "    loss.requires_grad_()\n",
        "\n",
        "    # compute gradient of loss w.r.t. logits\n",
        "    grad_loss = torch.autograd.grad(loss, y, retain_graph=True)[0]\n",
        "\n",
        "    # call your implemented \"grad_batch_norm\" function\n",
        "    grad_filter_1, grad_filter_2, grad_weight = grad_CNN(img, filter_1, filter_2, weight, grad_loss, channel_size=channel_size, num_filters=num_filters, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "    # compute ground-truth gradients\n",
        "    grad_filter_1_gt = torch.autograd.grad(loss, filter_1, retain_graph=True)[0]\n",
        "    grad_filter_2_gt = torch.autograd.grad(loss, filter_2, retain_graph=True)[0]\n",
        "    grad_weight_gt = torch.autograd.grad(loss, weight, retain_graph=True)[0]\n",
        "\n",
        "    diff = (grad_filter_1 - grad_filter_1_gt).norm()\n",
        "    if diff < 1.0e-5:\n",
        "        print(\"Your implementation of grad_filter_1 is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of grad_filter_1 is wrong!\")\n",
        "\n",
        "    diff = (grad_filter_2 - grad_filter_2_gt).norm()\n",
        "    if diff < 1.0e-5:\n",
        "        print(\"Your implementation of grad_filter_2 is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of grad_filter_2 is wrong!\")\n",
        "\n",
        "    diff = (grad_weight - grad_weight_gt).norm()\n",
        "    if diff < 1.0e-5:\n",
        "        print(\"Your implementation of grad_weight is correct!\")\n",
        "    else:\n",
        "        print(\"Your implementation of grad_weight is wrong!\")\n",
        "\n",
        "\n",
        "filter_1 = torch.randn(D, C, K, K) # filter shape: D x C x K x K\n",
        "filter_2 = torch.randn(D, D, K, K) # filter shape: D x C x K x K\n",
        "\n",
        "### compute the correct shape and then replace None with it ###\n",
        "weight = torch.randn(1568, 10) # weight of the last linear layer\n",
        "\n",
        "unit_test_CNN(img, label, filter_1, filter_2, weight, channel_size=C, num_filters=D, kernel_size=K, stride=1, padding=P)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}