{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Weifeng-Ke/Machine-Learning/blob/main/Programming_Assignment_2/CPEN455_2024W02_Assignment_2_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmGKp9MCGzEI",
        "outputId": "85c4d1b6-49b9-4fa6-fa14-6888e0f37824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/CPEN_455/Assignment_2\n"
          ]
        }
      ],
      "source": [
        "# # prompt: mount to google drive\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# %cd \"/content/drive/MyDrive/CPEN_455/Assignment_2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "fJmxtD-kg8KE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Eritefi9hPgk"
      },
      "outputs": [],
      "source": [
        "class SubstringDataset(Dataset):\n",
        "    LETTERS = list('cpen')\n",
        "\n",
        "    def __init__(self, seed, dataset_size, str_len=20):\n",
        "        super().__init__()\n",
        "        self.str_len = str_len\n",
        "        self.dataset_size = dataset_size\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "        self.strings, self.labels = self._create_dataset()\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.strings[index], self.labels[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size\n",
        "\n",
        "    def _create_dataset(self):\n",
        "        strings, labels = [], []\n",
        "        for i in range(self.dataset_size):\n",
        "            label = i%2\n",
        "            string = self._generate_random_string(bool(label))\n",
        "            strings.append(string)\n",
        "            labels.append(label)\n",
        "        return strings, labels\n",
        "\n",
        "    def _generate_random_string(self, has_cpen):\n",
        "        while True:\n",
        "            st = ''.join(self.rng.choice(SubstringDataset.LETTERS, size=self.str_len))\n",
        "            if ('cpen' in st) == has_cpen:\n",
        "                return st"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AY91aQytmtqK"
      },
      "outputs": [],
      "source": [
        "class Tokenizer():\n",
        "    def __init__(self) -> None:\n",
        "        self.vocab = {\n",
        "            '[CLS]': 0,\n",
        "            'c': 1,\n",
        "            'p': 2,\n",
        "            'e': 3,\n",
        "            'n': 4,\n",
        "        }\n",
        "\n",
        "    def tokenize_string(self, string, add_cls_token=True) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Tokenize the input string according to the above vocab\n",
        "\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "        tokens=[]\n",
        "        token_ids=[]\n",
        "        #take care the case where cls is included, if so put a 0 into the token_id\n",
        "        if add_cls_token:\n",
        "            tokens.append(self.vocab['[CLS]'])\n",
        "        #step 1 split the string and return a list of characters\n",
        "        #split the string into individual characters\n",
        "        tokens.extend(list(string))\n",
        "\n",
        "        #convert each token into corresponding token_id using the vocab\n",
        "        for token in tokens:\n",
        "          if token == 0:\n",
        "              token_ids.append(0)\n",
        "          else:\n",
        "              token_ids.append(self.vocab[token])\n",
        "\n",
        "        # creat one hot matrix for token ids\n",
        "        #the number of tokens is the length of the token_ids length which is n+1\n",
        "        num_tokens=len(token_ids)\n",
        "        #dvoc is the length of the volcabulary\n",
        "        dvoc=len(self.vocab)\n",
        "        #the one hot matrix has the size of R^(n+1)x dvoc\n",
        "        one_hot=torch.zeros(num_tokens,dvoc)\n",
        "\n",
        "        #convert the list of tokens into one hot\n",
        "        token_ids_tensor=torch.tensor(token_ids)\n",
        "        #this f.one_hot funtion takes in a token_id tensor as input, the second parameter is the numebr of parameter\n",
        "        one_hot=F.one_hot(token_ids_tensor,num_classes=dvoc)\n",
        "        #now the tokenized_string is a one hot encoding matrix\n",
        "        tokenized_string = one_hot\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return tokenized_string\n",
        "\n",
        "    def tokenize_string_batch(self, strings, add_cls_token=True):\n",
        "        X = []\n",
        "        for s in strings:\n",
        "            X.append(self.tokenize_string(s, add_cls_token=add_cls_token))\n",
        "        return torch.stack(X, dim=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AG_mI9VSj0XM"
      },
      "outputs": [],
      "source": [
        "class AbsolutePositionalEncoding(nn.Module):\n",
        "    MAX_LEN = 256\n",
        "    def __init__(self, d_model):\n",
        "        super().__init__()\n",
        "        self.W = nn.Parameter(torch.empty((self.MAX_LEN, d_model)))\n",
        "        nn.init.normal_(self.W)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            x: shape B x N x D\n",
        "        returns:\n",
        "            out: shape B x N x D\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "        #Adds the positional encoding to the input embedding X\n",
        "        # X has shape batch_size, sequence_length, and d_model\n",
        "        B,N,D=x.shape;\n",
        "        #the weight matix has the size of R ^(max_len X d_model) also we assume everying string is within max_len\n",
        "        #slicing the positonal encoding matrix, unsqueeze adds a batch dimension so the positional_encoding has the dimension of R^(Batch size X max_len X d_modeld)\n",
        "        positional_encoding=self.W[:N,:].unsqueeze(0)\n",
        "\n",
        "        #element wise adding rows of this matrix to their correspindong position in the input\n",
        "        out = x+positional_encoding\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    MAX_LEN = 256\n",
        "\n",
        "    def __init__(self, d_model, n_heads, rpe):\n",
        "        super().__init__()\n",
        "        assert d_model % n_heads == 0, \"Number of heads must divide number of dimensions\"\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.d_h = d_model // n_heads\n",
        "        self.rpe = rpe\n",
        "        self.Wq = nn.ParameterList([nn.Parameter(torch.empty((d_model, self.d_h))) for _ in range(n_heads)])\n",
        "        self.Wk = nn.ParameterList([nn.Parameter(torch.empty((d_model, self.d_h))) for _ in range(n_heads)])\n",
        "        self.Wv = nn.ParameterList([nn.Parameter(torch.empty((d_model, self.d_h))) for _ in range(n_heads)])\n",
        "        self.Wo = nn.Parameter(torch.empty((d_model, d_model)))\n",
        "\n",
        "        if rpe:\n",
        "            # -MAX_LEN, -MAX_LEN+1, ..., -1, 0, 1, ..., MAX_LEN-1, MAXLEN\n",
        "            self.rpe_w = nn.ParameterList([nn.Parameter(torch.empty((2*self.MAX_LEN+1, ))) for _ in range(n_heads)])\n",
        "\n",
        "        for h in range(self.n_heads):\n",
        "            nn.init.xavier_normal_(self.Wk[h])\n",
        "            nn.init.xavier_normal_(self.Wq[h])\n",
        "            nn.init.xavier_normal_(self.Wv[h])\n",
        "            if rpe:\n",
        "                nn.init.normal_(self.rpe_w[h])\n",
        "        nn.init.xavier_normal_(self.Wo)\n",
        "\n",
        "    def forward(self, key, query, value):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            key: shape B x N x D\n",
        "            query: shape B x N x D\n",
        "            value: shape B x N x D\n",
        "        return:\n",
        "            out: shape B x N x D\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "        B, N, D = query.shape\n",
        "        head_outputs = []  # To collect outputs from each head\n",
        "\n",
        "        for h in range(self.n_heads):\n",
        "            #first compute the X times Wq,Wk,and Wv\n",
        "            Q = torch.matmul(query, self.Wq[h])  # (B, N, d_h)\n",
        "            K = torch.matmul(key, self.Wk[h])      # (B, N, d_h)\n",
        "            V = torch.matmul(value, self.Wv[h])    # (B, N, d_h)\n",
        "\n",
        "            #feed thist into the attention block\n",
        "            # Compute scaled dot-product for Q times K.transpose: (B, N, N)\n",
        "            scores = torch.bmm(Q, K.transpose(1, 2))\n",
        "\n",
        "            #need to account for if the token are position encodede\n",
        "            if self.rpe:\n",
        "                # Create relative position indices: shape (N, N)\n",
        "                pos_indices = (\n",
        "                    torch.arange(N, device=query.device).unsqueeze(0) -\n",
        "                    torch.arange(N, device=query.device).unsqueeze(1)\n",
        "                )\n",
        "                # Shift indices to be non-negative: values in [0, 2*MAX_LEN]\n",
        "                pos_indices = pos_indices + self.MAX_LEN\n",
        "                # Lookup relative bias for head h: shape (N, N)\n",
        "                relative_bias = self.rpe_w[h][pos_indices]\n",
        "                # Expand to batch dimension and add to scores\n",
        "                scores = scores + relative_bias.unsqueeze(0)\n",
        "\n",
        "            # Scale scores by dividing the dimension of the head attention for statble gradiens\n",
        "            scores = scores / (self.d_h ** 0.5)\n",
        "            #Attention (Q,K,V)= softmax(Q*K^(T)/(sqrt(d_h)))*V; Compute attention weights with softmax: shape (B, N, N)\n",
        "            attention_weights = torch.softmax(scores, dim=-1)\n",
        "            #output = attention weight times the xv * wv\n",
        "            # Compute weighted sum of values: shape (B, N, d_h)\n",
        "            head_output = torch.bmm(attention_weights, V)\n",
        "            #once we get the head_output we need to concatenate these head_output\n",
        "            head_outputs.append(head_output)\n",
        "\n",
        "        # Concatenate outputs from all heads: shape (B, N, d_model)\n",
        "        concat = torch.cat(head_outputs, dim=-1)\n",
        "        #afger we have all the head_output we are going to compute attention\n",
        "        #attention (Xk,Xq,Xv) = concat(head1,head2,head3) * Wo\n",
        "        # Final linear projection: shape (B, N, d_model)\n",
        "        out = torch.matmul(concat, self.Wo)\n",
        "\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-wo7AXxYCV-U"
      },
      "outputs": [],
      "source": [
        "# # Instantiate Absolute Positional Encoding\n",
        "# d_model = 16  # Embedding dimension\n",
        "# seq_len = 10  # Sequence length\n",
        "# batch_size = 2  # Number of samples\n",
        "\n",
        "# # Create a dummy input tensor (random embeddings)\n",
        "# input_tensor = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "# # Initialize the Positional Encoding module\n",
        "# pos_encoding = AbsolutePositionalEncoding(d_model)\n",
        "\n",
        "# # Pass the input tensor through the positional encoding\n",
        "# output = pos_encoding(input_tensor)\n",
        "\n",
        "# # Check shape consistency\n",
        "# print(\"Input Shape:\", input_tensor.shape)  # Expected: (2, 10, 16)\n",
        "# print(\"Output Shape:\", output.shape)  # Expected: (2, 10, 16)\n",
        "# # Ensure positional encoding is being added (should be different from input)\n",
        "# print(\"Output Different from Input:\", not torch.allclose(input_tensor, output))\n",
        "\n",
        "# # Extract positional encodings applied to both sequences\n",
        "# pos_enc_1 = output[0] - input_tensor[0]  # Encoding for first batch element\n",
        "# pos_enc_2 = output[1] - input_tensor[1]  # Encoding for second batch element\n",
        "\n",
        "# # Check if positional encodings across different batches are the same\n",
        "# print(\"Same Positional Encoding for All Batches:\", torch.allclose(pos_enc_1, pos_enc_2))\n",
        "\n",
        "# # Create a tensor with zeros to isolate positional encoding effect\n",
        "# zero_tensor = torch.zeros(batch_size, seq_len, d_model)\n",
        "# pos_only_output = pos_encoding(zero_tensor)  # Only positional encoding remains\n",
        "\n",
        "# # Check if different positions have different encodings\n",
        "# pos_variation = torch.all(pos_only_output[:, 0, :] != pos_only_output[:, 1, :])\n",
        "# print(\"Different Positions Have Different Encodings:\", pos_variation)\n",
        "# # Extract encodings for the first position across different batches\n",
        "# pos_0_batch_1 = pos_only_output[0, 0, :]\n",
        "# pos_0_batch_2 = pos_only_output[1, 0, :]\n",
        "\n",
        "# # Check if encoding for position 0 is the same across batches\n",
        "# print(\"Same Encoding for Same Position Across Batches:\", torch.allclose(pos_0_batch_1, pos_0_batch_2))\n",
        "\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Extract encodings for visualization\n",
        "# pos_encoding_values = pos_only_output[0].detach().numpy()  # Take first batch\n",
        "\n",
        "# # Plot the positional encoding for the first few dimensions\n",
        "# plt.figure(figsize=(10, 6))\n",
        "# for i in range(min(4, d_model)):  # Plot first 4 dimensions\n",
        "#     plt.plot(range(seq_len), pos_encoding_values[:, i], label=f'Dim {i}')\n",
        "\n",
        "# plt.xlabel(\"Position in Sequence\")\n",
        "# plt.ylabel(\"Encoding Value\")\n",
        "# plt.title(\"Absolute Positional Encoding Across Different Positions\")\n",
        "# plt.legend()\n",
        "# plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used chat to explain to me how does pre-norm and post norm works and i reference the slide for the illustration."
      ],
      "metadata": {
        "id": "0LRHmLeONKY3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4iCqSqRDsGw3"
      },
      "outputs": [],
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, prenorm: bool, rpe: bool):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.prenorm = prenorm\n",
        "        self.attention = MultiHeadAttention(d_model, n_heads, rpe=rpe)\n",
        "        self.fc_W1 = nn.Parameter(torch.empty((d_model, 4*d_model)))\n",
        "        self.fc_W2 = nn.Parameter(torch.empty((4*d_model, d_model)))\n",
        "        self.relu = nn.ReLU()\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        nn.init.xavier_normal_(self.fc_W1)\n",
        "        nn.init.xavier_normal_(self.fc_W2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            x: shape B x N x D\n",
        "        returns:\n",
        "            out: shape B x N x D\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "        #following the pre-norm flow chart\n",
        "        if self.prenorm:\n",
        "            #first go through the layer norm\n",
        "            attention_input = self.ln1(x)\n",
        "            #coming out of the multi-head attention\n",
        "            attention_output = self.attention(attention_input, attention_input, attention_input)\n",
        "            #join 2 branch into 1, input + output of multi_head attention\n",
        "            x = x + attention_output\n",
        "            #frist go through the layer norm\n",
        "            feed_forward_input = self.ln2(x)\n",
        "            #the go thought the feed_forward network, the feed_forward with the has w layers, the first is the fully connected layer\n",
        "            #the first layer is x*w1 and then Relu(x*w1)\n",
        "            feed_forward_hidden = self.relu(torch.matmul(feed_forward_input, self.fc_W1))\n",
        "            #the output of relu times w2\n",
        "            feed_forward_output = torch.matmul(feed_forward_hidden, self.fc_W2)\n",
        "            #add the two branch back together\n",
        "            out = x + feed_forward_output\n",
        "        else:\n",
        "            #post norm\n",
        "            #first feed directly to multi-head attention\n",
        "            attention_output = self.attention(x, x, x)\n",
        "            #add the two branch together and then go though the layer norm\n",
        "            x=self.ln1(x+attention_output)\n",
        "\n",
        "            #go throught the FFN and the FFN has 2 layers, the first layer is X*W1 and then relu\n",
        "            feed_forward_hidden = self.relu(torch.matmul(x, self.fc_W1))\n",
        "            #the second layer is RElu()*W2\n",
        "            feed_forward_output = torch.matmul(feed_forward_hidden, self.fc_W2)\n",
        "            #add the two branch x+ ffn\n",
        "            out = self.ln2(x + feed_forward_output)\n",
        "\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I asked chat for hekp for this section, becasue i am not sure exactly how this encoder decoder model works. So i asked chatgpt, so it explained to me that this is just input -> encoder -> linear layer  and then -> decoder"
      ],
      "metadata": {
        "id": "p_TE0KcMvsJf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IxPXjEj1ydLf"
      },
      "outputs": [],
      "source": [
        "class ModelConfig:\n",
        "    n_layers = 4\n",
        "    input_dim = 5\n",
        "    d_model = 256\n",
        "    n_heads = 4\n",
        "    prenorm = True\n",
        "    pos_enc_type = 'ape' # 'ape': Abosolute Pos. Enc., 'rpe': Relative Pos. Enc.\n",
        "    output_dim = 1 # Binary output: 0: invalid, 1: valid\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            assert hasattr(self, k)\n",
        "            self.__setattr__(k, v)\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, cfg: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "        self.enc_W = nn.Parameter(torch.empty((cfg.input_dim, cfg.d_model)))\n",
        "        if cfg.pos_enc_type == 'ape':\n",
        "            self.ape = AbsolutePositionalEncoding(d_model=cfg.d_model)\n",
        "        self.transformer_layers = nn.ModuleList([\n",
        "            TransformerLayer(d_model=cfg.d_model, n_heads=cfg.n_heads, prenorm=cfg.prenorm, rpe=cfg.pos_enc_type == 'rpe') for _ in range(cfg.n_layers)\n",
        "        ])\n",
        "        self.dec_W = nn.Parameter(torch.empty((cfg.d_model, cfg.output_dim)))\n",
        "\n",
        "        nn.init.xavier_normal_(self.enc_W)\n",
        "        nn.init.xavier_normal_(self.dec_W)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            x: shape B x N x D_in\n",
        "        returns:\n",
        "            out: shape B x N x D_out\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "        #this is just fix the system from complaininig the data type of x is not matching the float type\n",
        "        x=x.type(torch.float32)\n",
        "        #xshape BxNXD_in time W shape d_in x d model-> B x N x d_model\n",
        "        #encoder project the input tokens from d_voc to d_model\n",
        "        x=torch.matmul(x,self.enc_W)\n",
        "        #apply the absolution positional encoding if the position needs encoding\n",
        "        if self.cfg.pos_enc_type == 'ape':\n",
        "            x=self.ape(x)\n",
        "        #apply the transformer layers\n",
        "        for layer in self.transformer_layers:\n",
        "            x=layer(x)\n",
        "        #conver x in B x N x D_model times W in d_model x D_out -> Out is in B x N x D_out\n",
        "        #decoder map each token from d_model to d_out\n",
        "        out = torch.matmul(x,self.dec_W)\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "i also used chat for this question, i ask chat what is lr_scheduler and what does it do."
      ],
      "metadata": {
        "id": "6VrZUet3yrhq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "k32Ps5WS9rg-"
      },
      "outputs": [],
      "source": [
        "from torch.optim import lr_scheduler\n",
        "\n",
        "class CustomScheduler(lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, total_steps, warmup_steps=1000):\n",
        "        self.total_steps = total_steps\n",
        "        self.warmup_steps = warmup_steps\n",
        "        super().__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        \"\"\"\n",
        "        Compute the custom scheduler with warmup and cooldown\n",
        "        Hint: self.last_epoch contains the current step number\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "\n",
        "        current_step = self.last_epoch\n",
        "        #if the current step is lower than the warmup step, speed up in a linear fashion\n",
        "        if current_step < self.warmup_steps:\n",
        "            mult_factor = current_step / self.warmup_steps\n",
        "        #if the current step has exceed warmup step and is lower than the max step, we need to slow down in a linear fashion\n",
        "        elif current_step <= self.total_steps:\n",
        "            mult_factor = (self.total_steps-current_step) / (self.total_steps - self.warmup_steps)\n",
        "        #if it reaches the max set the multi factor to be 0\n",
        "        else:\n",
        "            mult_factor = 0.0\n",
        "\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return [group['initial_lr'] * mult_factor for group in self.optimizer.param_groups]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "HmjFKAXcyeZm"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TrainerConfig:\n",
        "    lr = 0.003\n",
        "    train_steps = 5000\n",
        "    batch_size = 256\n",
        "    evaluate_every = 100\n",
        "    device = 'cpu'\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            assert hasattr(self, k)\n",
        "            self.__setattr__(k, v)\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, cfg: TrainerConfig):\n",
        "        self.cfg = cfg\n",
        "        self.device = cfg.device\n",
        "        self.tokenizer = Tokenizer()\n",
        "        self.model = model.to(self.device)\n",
        "\n",
        "    def train(self, train_dataset, val_dataset):\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=self.cfg.lr)\n",
        "        scheduler = CustomScheduler(optimizer, self.cfg.train_steps)\n",
        "        train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=self.cfg.batch_size)\n",
        "        for step in range(self.cfg.train_steps):\n",
        "            self.model.train()\n",
        "            batch = next(iter(train_dataloader))\n",
        "            strings, y = batch\n",
        "            x = self.tokenizer.tokenize_string_batch(strings)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss, _ = self.compute_batch_loss_acc(x, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            if step % self.cfg.evaluate_every == 0:\n",
        "                val_loss, val_acc = self.evaluate_dataset(val_dataset)\n",
        "                print(f\"Step {step}: Train Loss={loss.item()}, Val Loss: {val_loss}, Val Accuracy: {val_acc}\")\n",
        "\n",
        "    def compute_batch_loss_acc(self, x, y):\n",
        "        \"\"\"\n",
        "        Compute the loss and accuracy of the model on batch (x, y)\n",
        "        args:\n",
        "            x: B x N x D_in\n",
        "            y: B\n",
        "        return:\n",
        "            loss, accuracy\n",
        "        START BLOCK\n",
        "        \"\"\"\n",
        "        #forward pass through the model\n",
        "        #x: (X,N,D_in) to (B,N,D_out) N is the sequence length and the d_out is the output feature dimension per token\n",
        "        out= self.model(x.to(self.device))\n",
        "        #extract the output [CLS] token this leave the dimension to be (B,D_out)\n",
        "        cls_logics = out[:,0,:]\n",
        "        #this give the dimension of (B,)\n",
        "        cls_logics = cls_logics.squeeze(-1) #remove the last dimension\n",
        "\n",
        "        #compute the cross entropy loss\n",
        "        loss=F.binary_cross_entropy_with_logits(cls_logics,y.float().to(self.device))\n",
        "        #compute the prediciton labels, the threshold output is at 0.5\n",
        "        prediction= (torch.sigmoid(cls_logics)>0.5).float()\n",
        "        #calculate the accuracy by comparing prediciton with groud truth\n",
        "        acc=torch.mean((prediction==y.to(self.device)).float())\n",
        "        \"\"\"\n",
        "        END BLOCK\n",
        "        \"\"\"\n",
        "        return loss, acc\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate_dataset(self, dataset):\n",
        "        self.model.eval()\n",
        "        dataloader = DataLoader(dataset, shuffle=False, batch_size=self.cfg.batch_size)\n",
        "        final_loss, final_acc = 0.0, 0.0\n",
        "        for batch in dataloader:\n",
        "            strings, y = batch\n",
        "            x = self.tokenizer.tokenize_string_batch(strings)\n",
        "            loss, acc = self.compute_batch_loss_acc(x, y)\n",
        "            final_loss += loss.item() * x.size(0)\n",
        "            final_acc += acc.item() * x.size(0)\n",
        "        return final_loss / len(dataset), final_acc / len(dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "b5zfy4SVFy0V"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "In case you were not successful in implementing some of the above classes,\n",
        "you may reimplement them using pytorch available nn Modules here to receive the marks for part 1.8\n",
        "If your implementation of the previous parts is correct, leave this block empty.\n",
        "START BLOCK\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "END BLOCK\n",
        "\"\"\"\n",
        "def run_transformer():\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model = TransformerModel(ModelConfig())\n",
        "    trainer = Trainer(model, TrainerConfig(device=device))\n",
        "    parantheses_size=16\n",
        "    print(\"Creating datasets.\")\n",
        "    train_dataset = SubstringDataset(seed=1, dataset_size=10_000, str_len=parantheses_size)\n",
        "    val_dataset = SubstringDataset(seed=2, dataset_size=1_000, str_len=parantheses_size)\n",
        "    test_dataset = SubstringDataset(seed=3, dataset_size=1_000, str_len=parantheses_size)\n",
        "\n",
        "    print(\"Training the model.\")\n",
        "    trainer.train(train_dataset, val_dataset)\n",
        "    test_loss, test_acc = trainer.evaluate_dataset(test_dataset)\n",
        "    print(f\"Final Test Accuracy={test_acc}, Test Loss={test_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhAUyeO5F27T",
        "outputId": "c471c8ad-ea83-4016-d8b6-73c53120c062"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating datasets.\n",
            "Training the model.\n",
            "Step 0: Train Loss=0.8087093830108643, Val Loss: 0.7853522677421569, Val Accuracy: 0.4960000023841858\n",
            "Step 100: Train Loss=0.6845555305480957, Val Loss: 0.7267423634529113, Val Accuracy: 0.5029999976158142\n",
            "Step 200: Train Loss=0.6839892864227295, Val Loss: 0.6912579712867737, Val Accuracy: 0.5269999961853027\n",
            "Step 300: Train Loss=0.8801389336585999, Val Loss: 0.6900964970588684, Val Accuracy: 0.5329999961853027\n",
            "Step 400: Train Loss=0.7019082307815552, Val Loss: 0.7275902829170227, Val Accuracy: 0.5\n",
            "Step 500: Train Loss=0.6671653985977173, Val Loss: 0.7041060299873352, Val Accuracy: 0.5630000028610229\n",
            "Step 600: Train Loss=0.6978170871734619, Val Loss: 0.5994203486442566, Val Accuracy: 0.6679999971389771\n",
            "Step 700: Train Loss=0.4163946211338043, Val Loss: 0.28305449414253236, Val Accuracy: 0.8820000023841857\n",
            "Step 800: Train Loss=0.47237980365753174, Val Loss: 0.47560978651046754, Val Accuracy: 0.7930000009536743\n",
            "Step 900: Train Loss=0.5038537979125977, Val Loss: 0.44388349962234497, Val Accuracy: 0.7969999928474426\n",
            "Step 1000: Train Loss=0.4144652783870697, Val Loss: 0.3922766032218933, Val Accuracy: 0.8239999923706055\n",
            "Step 1100: Train Loss=0.2272491455078125, Val Loss: 0.21982781028747558, Val Accuracy: 0.9190000009536743\n",
            "Step 1200: Train Loss=0.21429364383220673, Val Loss: 0.22776675605773924, Val Accuracy: 0.9250000042915344\n",
            "Step 1300: Train Loss=0.18308430910110474, Val Loss: 0.2610315062999725, Val Accuracy: 0.8950000057220459\n",
            "Step 1400: Train Loss=0.15525653958320618, Val Loss: 0.21038796484470368, Val Accuracy: 0.9330000052452088\n",
            "Step 1500: Train Loss=0.05215135216712952, Val Loss: 0.11398018169403076, Val Accuracy: 0.9659999947547913\n",
            "Step 1600: Train Loss=0.21524851024150848, Val Loss: 0.2108303074836731, Val Accuracy: 0.9259999961853027\n",
            "Step 1700: Train Loss=0.1960340142250061, Val Loss: 0.29022590827941896, Val Accuracy: 0.9110000009536743\n",
            "Step 1800: Train Loss=0.22569936513900757, Val Loss: 0.19254353296756743, Val Accuracy: 0.9400000028610229\n",
            "Step 1900: Train Loss=0.1665164977312088, Val Loss: 0.17260282158851623, Val Accuracy: 0.9429999923706055\n",
            "Step 2000: Train Loss=0.07144343107938766, Val Loss: 0.10142520093917846, Val Accuracy: 0.9770000014305115\n",
            "Step 2100: Train Loss=0.03596479445695877, Val Loss: 0.13295570170879364, Val Accuracy: 0.9600000028610229\n",
            "Step 2200: Train Loss=0.10107829421758652, Val Loss: 0.10473294872045517, Val Accuracy: 0.9660000014305115\n",
            "Step 2300: Train Loss=0.389950156211853, Val Loss: 0.5075831217765808, Val Accuracy: 0.8100000009536743\n",
            "Step 2400: Train Loss=0.09346284717321396, Val Loss: 0.1464209623336792, Val Accuracy: 0.9569999980926513\n",
            "Step 2500: Train Loss=0.09563067555427551, Val Loss: 0.09784610986709595, Val Accuracy: 0.9719999957084656\n",
            "Step 2600: Train Loss=0.0528891384601593, Val Loss: 0.09347138410806656, Val Accuracy: 0.9729999990463257\n",
            "Step 2700: Train Loss=0.031869567930698395, Val Loss: 0.06774153172969818, Val Accuracy: 0.9840000047683716\n",
            "Step 2800: Train Loss=0.007104960735887289, Val Loss: 0.04782298675179481, Val Accuracy: 0.9850000047683716\n",
            "Step 2900: Train Loss=0.0017476925859227777, Val Loss: 0.05508573603630066, Val Accuracy: 0.9850000047683716\n",
            "Step 3000: Train Loss=0.015162769705057144, Val Loss: 0.045010252594947814, Val Accuracy: 0.9880000023841858\n",
            "Step 3100: Train Loss=0.010450499132275581, Val Loss: 0.08263593852519989, Val Accuracy: 0.9789999990463257\n",
            "Step 3200: Train Loss=0.001653887564316392, Val Loss: 0.05896003657579422, Val Accuracy: 0.9860000047683716\n",
            "Step 3300: Train Loss=0.0007923605153337121, Val Loss: 0.04695164394378662, Val Accuracy: 0.9860000047683716\n",
            "Step 3400: Train Loss=0.0029147167224437, Val Loss: 0.04763607314229012, Val Accuracy: 0.9879999966621399\n",
            "Step 3500: Train Loss=0.0005271910922601819, Val Loss: 0.05642676168680191, Val Accuracy: 0.9880000047683716\n",
            "Step 3600: Train Loss=0.00037197789060883224, Val Loss: 0.03278878143429756, Val Accuracy: 0.9920000023841858\n",
            "Step 3700: Train Loss=0.0002500687842257321, Val Loss: 0.03382386121153831, Val Accuracy: 0.9910000023841858\n",
            "Step 3800: Train Loss=6.797381502110511e-05, Val Loss: 0.03812969496846199, Val Accuracy: 0.9889999966621399\n",
            "Step 3900: Train Loss=0.000132527929963544, Val Loss: 0.037245237797498706, Val Accuracy: 0.9889999966621399\n",
            "Step 4000: Train Loss=7.163231202866882e-05, Val Loss: 0.03769583967328072, Val Accuracy: 0.9889999966621399\n",
            "Step 4100: Train Loss=6.189047417137772e-05, Val Loss: 0.03695222851634026, Val Accuracy: 0.9889999966621399\n",
            "Step 4200: Train Loss=4.1805877117440104e-05, Val Loss: 0.036329872965812686, Val Accuracy: 0.9889999966621399\n",
            "Step 4300: Train Loss=7.180919055826962e-05, Val Loss: 0.0370819953083992, Val Accuracy: 0.9889999966621399\n",
            "Step 4400: Train Loss=3.061158349737525e-05, Val Loss: 0.037546283811330794, Val Accuracy: 0.9889999966621399\n",
            "Step 4500: Train Loss=6.996859883656725e-05, Val Loss: 0.03794194445014, Val Accuracy: 0.9889999966621399\n",
            "Step 4600: Train Loss=6.043291068635881e-05, Val Loss: 0.0361302110850811, Val Accuracy: 0.9889999966621399\n",
            "Step 4700: Train Loss=4.0313687350135297e-05, Val Loss: 0.03641617766022682, Val Accuracy: 0.9889999966621399\n",
            "Step 4800: Train Loss=6.826993194408715e-05, Val Loss: 0.03635082477331161, Val Accuracy: 0.9889999966621399\n",
            "Step 4900: Train Loss=4.857787280343473e-05, Val Loss: 0.0361326347887516, Val Accuracy: 0.9889999966621399\n",
            "Final Test Accuracy=0.9940000023841858, Test Loss=0.034778636537492275\n"
          ]
        }
      ],
      "source": [
        "run_transformer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjNEOPRMsGKR"
      },
      "source": [
        "# Unit Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UjRY9u_UsFNm"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "\n",
        "def seed_all():\n",
        "    torch.manual_seed(0)\n",
        "    random.seed(0)\n",
        "    np.random.seed(0)\n",
        "\n",
        "class TransformerUnitTest:\n",
        "    def __init__(self, gt_vars: dict, verbose=False):\n",
        "        self.gt_vars = gt_vars\n",
        "        self.verbose = verbose\n",
        "\n",
        "    def test_all(self):\n",
        "        self.test_tokenizer()\n",
        "        self.test_ape()\n",
        "        self.test_mha()\n",
        "        self.test_transformer_layer()\n",
        "        self.test_transformer_model()\n",
        "        self.test_scheduler()\n",
        "        self.test_loss()\n",
        "\n",
        "    def test_tokenizer(self):\n",
        "        seed_all()\n",
        "        self.check_correctness(\n",
        "            Tokenizer().tokenize_string('ccpeen', add_cls_token=True),\n",
        "            self.gt_vars['tokenizer_1'],\n",
        "            \"Tokenization with cls class\"\n",
        "        )\n",
        "        self.check_correctness(\n",
        "            Tokenizer().tokenize_string('cpppencpen', add_cls_token=False),\n",
        "            self.gt_vars['tokenizer_2'],\n",
        "            \"Tokenization without cls class\"\n",
        "        )\n",
        "\n",
        "    def test_ape(self):\n",
        "        seed_all()\n",
        "        ape_result = AbsolutePositionalEncoding(128)(torch.randn((8, 12, 128)))\n",
        "        self.check_correctness(ape_result, self.gt_vars['ape'], \"APE\")\n",
        "\n",
        "    def test_mha(self):\n",
        "        seed_all()\n",
        "        mha_result = MultiHeadAttention(d_model=128, n_heads=4, rpe=False)(\n",
        "            torch.randn((8, 12, 128)), torch.randn((8, 12, 128)), torch.randn((8, 12, 128))\n",
        "        )\n",
        "        self.check_correctness(\n",
        "            mha_result,\n",
        "            self.gt_vars['mha_no_rpe'],\n",
        "            \"Multi-head Attention without RPE\"\n",
        "        )\n",
        "        mha_result_rpe = MultiHeadAttention(d_model=128, n_heads=8, rpe=True)(\n",
        "            torch.randn((8, 12, 128)), torch.randn((8, 12, 128)), torch.randn((8, 12, 128))\n",
        "        )\n",
        "        self.check_correctness(\n",
        "            mha_result_rpe,\n",
        "            self.gt_vars['mha_with_rpe'],\n",
        "            \"Multi-head Attention with RPE\"\n",
        "        )\n",
        "\n",
        "    def test_transformer_layer(self):\n",
        "        seed_all()\n",
        "        for prenorm in [True, False]:\n",
        "            transformer_layer_result = TransformerLayer(\n",
        "                d_model=128, n_heads=4, prenorm=prenorm, rpe=False\n",
        "            )(torch.randn((8, 12, 128)))\n",
        "            self.check_correctness(\n",
        "                transformer_layer_result,\n",
        "                self.gt_vars[f'transformer_layer_prenorm_{prenorm}'],\n",
        "                f\"Transformer Layer Prenorm {prenorm}\"\n",
        "            )\n",
        "\n",
        "    def test_transformer_model(self):\n",
        "        seed_all()\n",
        "        transformer_model_result = TransformerModel(\n",
        "            ModelConfig(d_model=128, prenorm=True, pos_enc_type='ape')\n",
        "        )(torch.randn((8, 12, 5)))\n",
        "        self.check_correctness(\n",
        "            transformer_model_result,\n",
        "            self.gt_vars['transformer_model_result'],\n",
        "            f\"Transformer Model\"\n",
        "        )\n",
        "\n",
        "    def test_scheduler(self):\n",
        "        model = TransformerModel(ModelConfig())\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        scheduler = CustomScheduler(optimizer, 10_000)\n",
        "        optimizer.step()\n",
        "        scheduler.step(521)\n",
        "        self.check_correctness(\n",
        "            torch.tensor([optimizer.param_groups[0]['lr']]),\n",
        "            self.gt_vars['scheduler_1'],\n",
        "            f\"Scheduler Warmup\"\n",
        "        )\n",
        "        scheduler.step(2503)\n",
        "        self.check_correctness(\n",
        "            torch.tensor([optimizer.param_groups[0]['lr']]),\n",
        "            self.gt_vars['scheduler_2'],\n",
        "            f\"Scheduler Cooldown\"\n",
        "        )\n",
        "\n",
        "    def test_loss(self):\n",
        "        seed_all()\n",
        "        model = TransformerModel(ModelConfig())\n",
        "        trainer = Trainer(model, TrainerConfig(device='cpu'))\n",
        "        loss_result, _ = trainer.compute_batch_loss_acc(\n",
        "            torch.randn((8, 12, 5)),\n",
        "            torch.ones(8).float(),\n",
        "        )\n",
        "        self.check_correctness(\n",
        "            loss_result,\n",
        "            self.gt_vars['loss'],\n",
        "            f\"Batch Loss\"\n",
        "        )\n",
        "\n",
        "    def check_correctness(self, out, gt, title):\n",
        "        try:\n",
        "            diff = (out - gt).norm()\n",
        "        except:\n",
        "            diff = float('inf')\n",
        "        if diff < 1e-4:\n",
        "            print(f\"[Correct] {title}\")\n",
        "        else:\n",
        "            print(f\"[Wrong] {title}\")\n",
        "            if self.verbose:\n",
        "                print(\"-----\")\n",
        "                print(\"Expected: \")\n",
        "                print(gt)\n",
        "                print(\"Received: \")\n",
        "                print(out)\n",
        "                print(\"-----\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "u2DlMVJ4wMrp"
      },
      "outputs": [],
      "source": [
        "#!gdown 1-2-__6AALEfqhfew3sJ2QiCE1-rrFMnQ -q -O unit_tests.pkl\n",
        "import pickle\n",
        "with open('unit_tests.pkl', 'rb') as f:\n",
        "    gt_vars = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Enp2ArbjOHEt",
        "outputId": "0e1e70de-672a-4cd2-dbd2-92f50bb24a8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Correct] Tokenization with cls class\n",
            "[Correct] Tokenization without cls class\n",
            "[Correct] APE\n",
            "[Correct] Multi-head Attention without RPE\n",
            "[Correct] Multi-head Attention with RPE\n",
            "[Correct] Transformer Layer Prenorm True\n",
            "[Correct] Transformer Layer Prenorm False\n",
            "[Correct] Transformer Model\n",
            "[Correct] Scheduler Warmup\n",
            "[Correct] Scheduler Cooldown\n",
            "[Correct] Batch Loss\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        }
      ],
      "source": [
        "TransformerUnitTest(gt_vars, verbose=False).test_all()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}